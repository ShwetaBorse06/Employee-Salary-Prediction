# Employee Salary Prediction and Analysis

# ====================
# 1. Import Libraries
# ====================
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# ============================
# 2. Load the Dataset
# ============================
df = pd.read_csv(r"C:\Users\Admin\Downloads\Salary Data.csv")  # Adjust path as needed

# ============================
# 3. Clean the Data
# ============================
df.dropna(inplace=True)

# ============================
# 4. Feature and Target Split
# ============================
X = df.drop("Salary", axis=1)
y = df["Salary"]

# ============================
# 5. Column Identification
# ============================
categorical_cols = ["Gender", "Education Level", "Job Title"]
numerical_cols = ["Age", "Years of Experience"]

# ============================
# 6. Preprocessing Pipeline
# ============================
preprocessor = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(drop="first", handle_unknown="ignore"), categorical_cols)
    ],
    remainder="passthrough"  # Keep numerical features unchanged
)

# ============================
# 7. Model Definition
# ============================
model = RandomForestRegressor(random_state=42)

# ============================
# 8. Full Pipeline
# ============================
pipeline = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("model", model)
])

# ============================
# 9. Train-Test Split
# ============================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ============================
# 10. Train the Model
# ============================
pipeline.fit(X_train, y_train)

# ============================
# 11. Make Predictions
# ============================
y_pred = pipeline.predict(X_test)

# ============================
# 12. Evaluate the Model
# ============================
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
print(f"RÂ² Score: {r2:.2f}")

# ============================
# 13. Visualizations
# ============================

# Scatter plot: Actual vs Predicted
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, color='teal', alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')
plt.xlabel("Actual Salary")
plt.ylabel("Predicted Salary")
plt.title("Actual vs Predicted Salaries")
plt.grid(True)
plt.tight_layout()
plt.show()

# Residuals plot
residuals = y_test - y_pred
plt.figure(figsize=(8, 6))
sns.histplot(residuals, kde=True, color='purple')
plt.title("Distribution of Residuals (Prediction Errors)")
plt.xlabel("Residual (Actual - Predicted)")
plt.ylabel("Frequency")
plt.grid(True)
plt.tight_layout()
plt.show()

# Feature Importance plot
feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out(X.columns)
importances = pipeline.named_steps['model'].feature_importances_
feat_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
feat_df = feat_df.sort_values(by="Importance", ascending=False).head(10)

plt.figure(figsize=(10, 6))
sns.barplot(x="Importance", y="Feature", data=feat_df, color="skyblue")
plt.title("Top 10 Important Features in Salary Prediction")
plt.tight_layout()
plt.show()
